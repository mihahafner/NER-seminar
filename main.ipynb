{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mihahafner/NER-seminar/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Google Drive folder\n",
        "\n",
        "https://drive.google.com/drive/folders/1H7yLbPVODNG5ANgT2BWA5Y-xU6bhAAcf?usp=sharing\n",
        "\n",
        "# Dataset source\n",
        "\n",
        "https://www.kaggle.com/datasets/abhinavwalia95/entity-annotated-corpus"
      ],
      "metadata": {
        "id": "DX4Q7ulmZF3W"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yq2mId-QgOOG"
      },
      "source": [
        "# SETUP"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# POVEZEMO SE NA GOOGLE DRIVE KJER SE SHRANJUJEJO VSE PODATKI\n",
        "drive.mount('/content/drive')\n",
        "ROOT = '/content/drive/MyDrive/bert'\n",
        "\n",
        "# AUTORIZACIJA ZA KAGGLE \n",
        "os.environ['KAGGLE_USERNAME'] = 'mihahafner'\n",
        "os.environ['KAGGLE_KEY'] = '74203e91e15b3c38bee230d76f2a710e'\n",
        "\n",
        "# DOWNLOAD DATASET ZA TRENIRANJE\n",
        "!kaggle datasets download -d abhinavwalia95/entity-annotated-corpus\n",
        "!kaggle datasets download -d abhishek/bert-base-uncased\n",
        "\n",
        "# PRENOS ZIP-OV IZ KAGGLE V DRIVE\n",
        "!unzip -o entity-annotated-corpus.zip -d /content/drive/MyDrive/bert/entity-annotated-corpus\n",
        "!unzip -o bert-base-uncased.zip -d /content/drive/MyDrive/bert/bert-base-uncased\n",
        "\n",
        "# ISTALACIJA IN PRIPRAVA PRED TRENIRANJEM\n",
        "!pip install transformers\n",
        "!mkdir /content/drive/MyDrive/bert/models"
      ],
      "metadata": {
        "id": "x8hibfrbgynj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f23e75b8-0ca2-4ab6-f108-1a5d1fadfe62"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Downloading entity-annotated-corpus.zip to /content\n",
            " 72% 19.0M/26.4M [00:00<00:00, 195MB/s]\n",
            "100% 26.4M/26.4M [00:00<00:00, 225MB/s]\n",
            "Downloading bert-base-uncased.zip to /content\n",
            " 99% 386M/389M [00:01<00:00, 311MB/s]\n",
            "100% 389M/389M [00:01<00:00, 295MB/s]\n",
            "Archive:  entity-annotated-corpus.zip\n",
            "  inflating: /content/drive/MyDrive/bert/entity-annotated-corpus/ner.csv  \n",
            "  inflating: /content/drive/MyDrive/bert/entity-annotated-corpus/ner_dataset.csv  \n",
            "Archive:  bert-base-uncased.zip\n",
            "  inflating: /content/drive/MyDrive/bert/bert-base-uncased/config.json  \n",
            "  inflating: /content/drive/MyDrive/bert/bert-base-uncased/pytorch_model.bin  \n",
            "  inflating: /content/drive/MyDrive/bert/bert-base-uncased/vocab.txt  \n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.22.0-py3-none-any.whl (4.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 4.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Collecting huggingface-hub<1.0,>=0.9.0\n",
            "  Downloading huggingface_hub-0.9.1-py3-none-any.whl (120 kB)\n",
            "\u001b[K     |████████████████████████████████| 120 kB 64.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 53.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.9.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.9.1 tokenizers-0.12.1 transformers-4.22.0\n",
            "mkdir: cannot create directory ‘/content/drive/MyDrive/bert/models’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CONFIG"
      ],
      "metadata": {
        "id": "xmwea4LXRbYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import csv\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn import model_selection\n",
        "\n",
        "from tqdm import tqdm\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "import transformers\n",
        "from torch.optim import Adadelta, Adagrad, Adam, Adamax, AdamW, ASGD, LBFGS, NAdam, Rprop, SGD, RMSprop, RAdam, SparseAdam\n",
        "\n",
        "# CELOTNA KONFIGURACIJA PROJEKTA\n",
        "class config:\n",
        "\n",
        "    # HIRERPARAMETRI\n",
        "    TRAIN_BATCH_SIZE = 4 # 2, 4, 8, 16, 32, 64\n",
        "    EPOCHS = 50\n",
        "    OPTIMIZER = Adam\n",
        "    LEARN_RATE = 0.0001 # 0.001, 0.0001, 0.00001, 0.000001\n",
        "\n",
        "    # IME MODELA\n",
        "    MODEL = f'{OPTIMIZER.__name__}_batch-{TRAIN_BATCH_SIZE}_epoch-{EPOCHS}_learn-{str(LEARN_RATE)[2:]}'\n",
        "\n",
        "    # TRENING KONFIGURACIJA\n",
        "    MAX_LEN = 128\n",
        "    VALID_BATCH_SIZE = 8\n",
        "    BASE_MODEL_PATH = f\"{ROOT}/bert-base-uncased/\"\n",
        "    TRAINING_FILE = f\"{ROOT}/entity-annotated-corpus/ner_dataset.csv\"\n",
        "\n",
        "    # TOKENIZER\n",
        "    TOKENIZER = transformers.BertTokenizer.from_pretrained(\n",
        "        BASE_MODEL_PATH,\n",
        "        do_lower_case=True    \n",
        "    )\n",
        "\n",
        "# OBJEKT ZA LAZJO PRIPRAVO PRED TRENIRANJEM\n",
        "class EntityDataset:\n",
        "\n",
        "    # KONSTRUKTOR\n",
        "    def __init__(self, texts, pos, tags):\n",
        "        self.texts = texts\n",
        "        self.pos = pos\n",
        "        self.tags = tags\n",
        "    \n",
        "    # FUNKCIJA ZA IZACUN VELIKOSTI PODATKOV\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "    \n",
        "    # FUNKCIJA ZA PRIDOBIVANJE PODATKOV\n",
        "    def __getitem__(self, item):\n",
        "        text = self.texts[item]\n",
        "        pos = self.pos[item]\n",
        "        tags = self.tags[item]\n",
        "\n",
        "        ids = []\n",
        "        target_pos = []\n",
        "        target_tag =[]\n",
        "\n",
        "        for i, s in enumerate(text):\n",
        "            inputs = config.TOKENIZER.encode(\n",
        "                s,\n",
        "                add_special_tokens=False\n",
        "            )\n",
        "            # abhishek: ab ##hi ##sh ##ek\n",
        "            input_len = len(inputs)\n",
        "            ids.extend(inputs)\n",
        "            target_pos.extend([pos[i]] * input_len)\n",
        "            target_tag.extend([tags[i]] * input_len)\n",
        "\n",
        "        ids = ids[:config.MAX_LEN - 2]\n",
        "        target_pos = target_pos[:config.MAX_LEN - 2]\n",
        "        target_tag = target_tag[:config.MAX_LEN - 2]\n",
        "\n",
        "        ids = [101] + ids + [102]\n",
        "        target_pos = [0] + target_pos + [0]\n",
        "        target_tag = [0] + target_tag + [0]\n",
        "\n",
        "        mask = [1] * len(ids)\n",
        "        token_type_ids = [0] * len(ids)\n",
        "\n",
        "        padding_len = config.MAX_LEN - len(ids)\n",
        "\n",
        "        ids = ids + ([0] * padding_len)\n",
        "        mask = mask + ([0] * padding_len)\n",
        "        token_type_ids = token_type_ids + ([0] * padding_len)\n",
        "        target_pos = target_pos + ([0] * padding_len)\n",
        "        target_tag = target_tag + ([0] * padding_len)\n",
        "\n",
        "        # VRACAJOCI PODATKI ZA TRENIRANJE IN VALIDACIJO\n",
        "        return {\n",
        "            \"ids\": torch.tensor(ids, dtype=torch.long),\n",
        "            \"mask\": torch.tensor(mask, dtype=torch.long),\n",
        "            \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
        "            \"target_pos\": torch.tensor(target_pos, dtype=torch.long),\n",
        "            \"target_tag\": torch.tensor(target_tag, dtype=torch.long),\n",
        "        }\n",
        "\n",
        "# TRENING FUNKCIJO\n",
        "def train_fn(data_loader, model, optimizer, device, scheduler):\n",
        "    model.train()\n",
        "    final_loss = 0\n",
        "    losses = []\n",
        "    for data in tqdm(data_loader, total=len(data_loader)):\n",
        "        for k, v in data.items():\n",
        "            data[k] = v.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        _, _, loss = model(**data)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        final_loss += loss.item()\n",
        "        losses.append(loss.item())\n",
        "    \n",
        "    # VRACA SE SPISEK IZGUB IN POVPRECNA IZGUBA MED TRENIRANJEM\n",
        "    return losses, final_loss / len(data_loader)\n",
        "\n",
        "# EVALUACIJSKA FUNKCIJA\n",
        "def eval_fn(data_loader, model, device):\n",
        "    model.eval()\n",
        "    final_loss = 0\n",
        "    losses = []\n",
        "    for data in tqdm(data_loader, total=len(data_loader)):\n",
        "        for k, v in data.items():\n",
        "            data[k] = v.to(device)\n",
        "        _, _, loss = model(**data)\n",
        "        losses.append(loss.item())\n",
        "        final_loss += loss.item()\n",
        "    # VRACA SE SPISEK IZGUB IN POVPRECNA IZGUBA MED TRENIRANJEM\n",
        "    return losses, final_loss / len(data_loader)\n",
        "\n",
        "# FUNKCIJA ZA IZRACUN IZGUBE (TIP IZRACUNA JE CROSS ENTROPY)\n",
        "def loss_fn(output, target, mask, num_labels):\n",
        "    lfn = nn.CrossEntropyLoss()\n",
        "    active_loss = mask.view(-1) == 1\n",
        "    active_logits = output.view(-1, num_labels)\n",
        "    active_labels = torch.where(\n",
        "        active_loss,\n",
        "        target.view(-1),\n",
        "        torch.tensor(lfn.ignore_index).type_as(target)\n",
        "    )\n",
        "    loss = lfn(active_logits, active_labels)\n",
        "    return loss\n",
        "\n",
        "# DEFINICIJA MODELA KI SE TRENIRA\n",
        "class EntityModel(nn.Module):\n",
        "    def __init__(self, num_tag, num_pos):\n",
        "        super(EntityModel, self).__init__()\n",
        "        self.num_tag = num_tag\n",
        "        self.num_pos = num_pos\n",
        "\n",
        "        # TRENIRA SE MODEL BERT...\n",
        "        self.bert = transformers.BertModel.from_pretrained(\n",
        "            config.BASE_MODEL_PATH, return_dict=False\n",
        "        )\n",
        "\n",
        "        # NOTRANJE HIPERPARAMETRE MODELA\n",
        "        self.bert_drop_1 = nn.Dropout(0.3)\n",
        "        self.bert_drop_2 = nn.Dropout(0.3)\n",
        "\n",
        "        # VELIKOST VHODNIH PODATKOV\n",
        "        self.out_tag = nn.Linear(768, self.num_tag)\n",
        "        self.out_pos = nn.Linear(768, self.num_pos)\n",
        "    \n",
        "    # FORWARD PASS\n",
        "    def forward(\n",
        "        self, \n",
        "        ids, \n",
        "        mask, \n",
        "        token_type_ids, \n",
        "        target_pos, \n",
        "        target_tag\n",
        "    ):\n",
        "        o1, _ = self.bert(\n",
        "            ids, \n",
        "            attention_mask=mask, \n",
        "            token_type_ids=token_type_ids\n",
        "        )\n",
        "\n",
        "        bo_tag = self.bert_drop_1(o1)\n",
        "        bo_pos = self.bert_drop_2(o1)\n",
        "\n",
        "        tag = self.out_tag(bo_tag)\n",
        "        pos = self.out_pos(bo_pos)\n",
        "\n",
        "        loss_tag = loss_fn(tag, target_tag, mask, self.num_tag)\n",
        "        loss_pos = loss_fn(pos, target_pos, mask, self.num_pos)\n",
        "\n",
        "        loss = (loss_tag + loss_pos) / 2\n",
        "\n",
        "        return tag, pos, loss\n",
        "\n",
        "# PREDPRIPRAVA PODATKOV V PODATKOVNE STRUKTURE\n",
        "def process_data(data_path):\n",
        "    df = pd.read_csv(data_path, encoding=\"latin-1\")\n",
        "    df.loc[:, \"Sentence #\"] = df[\"Sentence #\"].fillna(method=\"ffill\")\n",
        "\n",
        "    enc_pos = preprocessing.LabelEncoder()\n",
        "    enc_tag = preprocessing.LabelEncoder()\n",
        "\n",
        "    df.loc[:, \"POS\"] = enc_pos.fit_transform(df[\"POS\"])\n",
        "    df.loc[:, \"Tag\"] = enc_tag.fit_transform(df[\"Tag\"])\n",
        "\n",
        "    sentences = df.groupby(\"Sentence #\")[\"Word\"].apply(list).values\n",
        "    pos = df.groupby(\"Sentence #\")[\"POS\"].apply(list).values\n",
        "    tag = df.groupby(\"Sentence #\")[\"Tag\"].apply(list).values\n",
        "    return sentences, pos, tag, enc_pos, enc_tag\n",
        "\n",
        "# PRIDOBITEV PODATKOVNIH STRUKTUR IZ SUROVIH CSV FILEOV.\n",
        "sentences, pos, tag, enc_pos, enc_tag = process_data(config.TRAINING_FILE)\n",
        "\n",
        "# SHRANJEVANJE META PODATKOV DATASETA.\n",
        "meta_data = {\n",
        "    \"enc_pos\": enc_pos,\n",
        "    \"enc_tag\": enc_tag\n",
        "}\n",
        "\n",
        "joblib.dump(meta_data, \"meta.bin\")"
      ],
      "metadata": {
        "id": "q5HShLuNRZQV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9beaae75-5f80-4c83-87b0-2dbfa8524e1a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['meta.bin']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdE5Som1gOOJ"
      },
      "source": [
        "# TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "AXv76WR1gOOL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69810d72-6283-42ad-fb5f-b6c8d3f4f474"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Some weights of the model checkpoint at /content/drive/MyDrive/bert/bert-base-uncased/ were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "100%|██████████| 10791/10791 [15:29<00:00, 11.61it/s]\n",
            "100%|██████████| 600/600 [00:24<00:00, 24.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss = 0.20994903117704133 Valid Loss = 0.16930577599133054\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10791/10791 [15:46<00:00, 11.40it/s]\n",
            "100%|██████████| 600/600 [00:27<00:00, 21.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss = 0.1757203314627528 Valid Loss = 0.16002733768274388\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10791/10791 [16:37<00:00, 10.82it/s]\n",
            "100%|██████████| 600/600 [00:29<00:00, 20.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss = 0.1643971791187117 Valid Loss = 0.15089564759905139\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 90%|████████▉ | 9689/10791 [14:56<01:34, 11.67it/s]"
          ]
        }
      ],
      "source": [
        "# IZRACUN STEVIL RAZREDOV KI SE NAHAJAJO V DATASETU\n",
        "num_pos = len(list(enc_pos.classes_))\n",
        "num_tag = len(list(enc_tag.classes_))\n",
        "\n",
        "# RAZDELITEV DATASETA NA TRENING IN TEST DATASET\n",
        "(\n",
        "    train_sentences,\n",
        "    test_sentences,\n",
        "    train_pos,\n",
        "    test_pos,\n",
        "    train_tag,\n",
        "    test_tag) = model_selection.train_test_split(\n",
        "    sentences, \n",
        "    pos, \n",
        "    tag, \n",
        "    random_state=42, # SEME NAKLJUCNOSTI\n",
        "    test_size=0.1 # KOLIKO PROCENTOV BO V TESTING DATASETU\n",
        ")\n",
        "\n",
        "# USTVARIMO Z PREDEFINIRANIM RAZREDOM OBJEKTE ZA LAZJO MANIPULACIJO...\n",
        "train_dataset = EntityDataset(\n",
        "    texts=train_sentences, pos=train_pos, tags=train_tag\n",
        ")\n",
        "\n",
        "train_data_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=config.TRAIN_BATCH_SIZE, num_workers=4\n",
        ")\n",
        "\n",
        "valid_dataset = EntityDataset(\n",
        "    texts=test_sentences, pos=test_pos, tags=test_tag\n",
        ")\n",
        "\n",
        "valid_data_loader = torch.utils.data.DataLoader(\n",
        "    valid_dataset, batch_size=config.VALID_BATCH_SIZE, num_workers=1\n",
        ")\n",
        "\n",
        "# PRIDOBIMO GPU KI BO IZVAJAL TRENIRANJE\n",
        "device = torch.device(\"cuda\")\n",
        "# USTVARIMO MODEL\n",
        "model = EntityModel(num_tag=num_tag, num_pos=num_pos)\n",
        "# NALOZIMO MODEL V GPU\n",
        "model.to(device)\n",
        "\n",
        "# DEFINIRAMO DODATNE PARAMETRE MODELA IN OPTIMIZACIJSKEGA ALGORITMA\n",
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
        "optimizer_parameters = [\n",
        "    {\n",
        "        \"params\": [\n",
        "            p for n, p in param_optimizer if not any(\n",
        "                nd in n for nd in no_decay\n",
        "            )\n",
        "        ],\n",
        "        \"weight_decay\": 0.001,\n",
        "    },\n",
        "    {\n",
        "        \"params\": [\n",
        "            p for n, p in param_optimizer if any(\n",
        "                nd in n for nd in no_decay\n",
        "            )\n",
        "        ],\n",
        "        \"weight_decay\": 0.0,\n",
        "    },\n",
        "]\n",
        "\n",
        "# IZRACUNAMO STEVILO TRENING KORAKOV (ITERACIJ)\n",
        "num_train_steps = int(\n",
        "    len(train_sentences) / config.TRAIN_BATCH_SIZE * config.EPOCHS\n",
        ")\n",
        "\n",
        "# USTVARIMO ALTORITEM KI BO OPTIMIZIRAL MODEL\n",
        "optimizer = config.OPTIMIZER(optimizer_parameters, lr=config.LEARN_RATE)\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer, \n",
        "    num_warmup_steps=0, \n",
        "    num_training_steps=num_train_steps\n",
        ")\n",
        "\n",
        "best_loss = np.inf\n",
        "all_train_losses = []\n",
        "all_test_losses = []\n",
        "\n",
        "# GLAVNA TRENING ZANKA\n",
        "for epoch in range(config.EPOCHS):\n",
        "\n",
        "    # IZVEDE SE TRENING FUNKCIJA\n",
        "    train_losses, train_loss = train_fn(\n",
        "        train_data_loader, \n",
        "        model, \n",
        "        optimizer, \n",
        "        device, \n",
        "        scheduler\n",
        "    )\n",
        "    # IZVEDE SE EVALUACIJSKA FUNKCIJA\n",
        "    test_losses, test_loss = eval_fn(\n",
        "        valid_data_loader,\n",
        "        model,\n",
        "        device\n",
        "    )\n",
        "    \n",
        "    # SHRANIMO INFORMACIJE TRENIRANJA V SPREMENLJIVKE\n",
        "    all_train_losses += train_losses\n",
        "    all_test_losses += test_losses\n",
        "\n",
        "    print(f\"Train Loss = {train_loss} Valid Loss = {test_loss}\")\n",
        "    \n",
        "    # Ce najdemo model ki ima manjsi testing loss ga shranimo v datoteko.\n",
        "    if test_loss < best_loss:\n",
        "        torch.save(model.state_dict(), f'{ROOT}/models/{config.MODEL}.torch')\n",
        "        best_loss = test_loss\n",
        "\n",
        "# Na koncu treniranja shranimo vse informacije o treniranju v csv datoteko.\n",
        "with open(f'{ROOT}/models/{config.MODEL}.csv', 'w') as f:\n",
        "    writer = csv.DictWriter(f, ['train', 'validation', 'batch'])\n",
        "    writer.writeheader()\n",
        "    for i in range(len(all_test_losses)):\n",
        "        writer.writerow({\n",
        "            'batch': i,\n",
        "            'train': all_train_losses[i],\n",
        "            'validation': all_test_losses[i]\n",
        "        })\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GRAPHS"
      ],
      "metadata": {
        "id": "4qgP3HeO7t3g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "import csv\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "data = []\n",
        "# Sprehodimo se po direktoriju kjer se nahajajo modeli.\n",
        "for fileName in os.listdir(f'{ROOT}/models'):\n",
        "\n",
        "    # Zanima nas samo csv/ji\n",
        "    if fileName.endswith('csv'):\n",
        "\n",
        "        # Pridobimo inormacije o csv datoteki iz vsebine naslova datoteke.\n",
        "        info = fileName.replace('.csv', '').split('_')\n",
        "\n",
        "        # Preberemo csv datoteko in shranimo informacije v spisek slovarjev\n",
        "        with open(f'{ROOT}/models/{fileName}', 'r') as file:\n",
        "            rows = []\n",
        "            for line in csv.DictReader(file):\n",
        "                rows.append({key: float(value) for key, value in line.items()})\n",
        "            \n",
        "            \n",
        "            # Sparsa se informacija o learning ratue.\n",
        "            info[3] = info[3].replace('learn-', '')\n",
        "            if info[3].startswith('-'):\n",
        "              info[3] = '1e' + info[3]\n",
        "            else:\n",
        "              info[3] = '0.' + info[3]\n",
        "            \n",
        "            # Informacija o csv datoteki se shrani v slovar spiskov.\n",
        "            data.append({\n",
        "                'file': fileName,\n",
        "                'optimizer': info[0],\n",
        "                'learn_rate': float(info[3]),\n",
        "                'batch_size': int(info[1].replace('batch-', '')),\n",
        "                'epochs': int(info[2].replace('epoch-', '')),\n",
        "                'rows': rows\n",
        "            })\n",
        "\n",
        "# Ce direktorij za grafe obstaja izbrisemo vse informacije notri.\n",
        "try:\n",
        "    shutil.rmtree(f'{ROOT}/graphs')\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Direktorij se se enkrat ustvari.\n",
        "os.mkdir(f'{ROOT}/graphs')\n",
        "\n",
        "legend = []\n",
        "scatter_x = []\n",
        "scatter_y = []\n",
        "scatter_area = []\n",
        "scatter_model = []\n",
        "\n",
        "# Pridobimo vse informacije iz csv pretvorjene oblike v grave in nove procesirane csv/je\n",
        "for model in data:\n",
        "\n",
        "    file_name = model['file']\n",
        "    opti = model['optimizer']\n",
        "    lr = model['learn_rate']\n",
        "    batch = model['batch_size']\n",
        "    rows = model['rows']\n",
        "    \n",
        "    x = [row['batch'] for row in rows]\n",
        "    val_loss = [row['validation'] for row in rows]\n",
        "    train_loss = [row['train'] for row in rows]\n",
        "    \n",
        "    scatter_x.append(batch)\n",
        "    scatter_y.append(lr)\n",
        "    scatter_model.append(opti)\n",
        "    scatter_area.append(val_loss[-1])\n",
        "    legend.append(f'{opti} LR{lr:.2E} batch {batch}')\n",
        "\n",
        "    path = f'{ROOT}/graphs/{file_name.replace(\".csv\", \"\")}'\n",
        "    os.mkdir(path)\n",
        "\n",
        "    plt.title(f'Optimizer: {opti}, Learn rate: {lr}, Batch size: {batch}')\n",
        "    plt.plot(x, val_loss)\n",
        "    plt.plot(x, train_loss)\n",
        "    plt.legend(['Validation loss', 'Train loss'])\n",
        "    plt.xlabel(\"Batch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.savefig(f'{path}/loss.png')\n",
        "    plt.clf()\n",
        "    plt.cla()\n",
        "\n",
        "\n",
        "for model in data:\n",
        "    rows = model['rows']\n",
        "    x = [row['batch'] for row in rows]\n",
        "    val_loss = [row['validation'] for row in rows]\n",
        "    plt.plot(x, val_loss)\n",
        "\n",
        "path = f'{ROOT}/graphs/all'\n",
        "os.mkdir(path)\n",
        "\n",
        "plt.title(f'All models validation loss functions')\n",
        "plt.legend(legend, loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "plt.xlabel(\"Batch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.tight_layout(rect=[0,0, 1, 1])\n",
        "plt.savefig(f'{path}/validation_loss.png')\n",
        "plt.clf()\n",
        "plt.cla()\n",
        "\n",
        "with open(f'{path}/learnRate_batch.csv', 'w') as file:\n",
        "    csvFile = csv.DictWriter(file, fieldnames=['model', 'learn_rate', 'batch_size', 'val_loss'])\n",
        "    csvFile.writeheader()\n",
        "    for i in range(len(scatter_y)):\n",
        "        csvFile.writerow({\n",
        "            'model': scatter_model[i],\n",
        "            'learn_rate': scatter_y[i],\n",
        "            'batch_size': scatter_x[i],\n",
        "            'val_loss': scatter_area[i],\n",
        "        })"
      ],
      "metadata": {
        "id": "_gtwUb8y7tWM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "c96c3013-b66b-4090-bf47-6c7c292b7a09"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAACcCAYAAACEEbwiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAJSUlEQVR4nO3df6jddR3H8edLl0lrunATRJ1T2tKbBa5DLYRcuGIu0D8scSBlDIdmEihBsTDRvywyEFZ2I5kKLqd/xAUnRjYZiFe9Yzp1kcwf1VLanMt/xKX07o/vd3V3vHfne+/9nHPe+X094ML58Tnf8+Lcve4553u+ex9FBGaWz3HDDmBmU3M5zZJyOc2ScjnNknI5zZJyOc2S6llOSXdL2i/phWmul6Q7Je2VtFvSivIxzdqnyTPnZmDNMa6/BFhW/2wAfjn3WGbWs5wRsQN46xhLLgPujco4sFDSaaUCmrVVifecpwN/m3R+X32Zmc3BvEHemaQNVC99mT9//ufOPffcQd692VDs3LnzzYhYPNPblSjn34EzJ50/o77sAyJiFBgF6HQ6MTExUeDuzXKT9JfZ3K7Ey9ox4Jv1XtuVwNsR8UaB7Zq1Ws9nTklbgFXAIkn7gB8DHwGIiLuAbcBaYC/wDvDtfoU1a5Oe5YyIdT2uD+D6YonMDPARQmZpuZxmSbmcZkm5nGZJuZxmSbmcZkm5nGZJuZxmSbmcZkm5nGZJuZxmSbmcZkm5nGZJuZxmSbmcZkk1KqekNZL+XM+m/cEU1y+RtF3Srnp27dryUc3apclQ6eOBTVTzaUeAdZJGupb9CNgaERcAVwK/KB3UrG2aPHN+HtgbEa9ExL+A31LNqp0sgJPq0ycDr5eLaNZOTabvTTWX9gtda24Bfi/pBmA+sLpIOrMWK7VDaB2wOSLOoBr2dZ+kD2xb0gZJE5ImDhw4UOiuzT6cmpSzyVza9cBWgIh4EjgRWNS9oYgYjYhORHQWL57xjF2zVmlSzmeAZZLOlnQC1Q6fsa41fwUuBpB0HlU5/dRoNgdNvsjofeC7wKPAn6j2yr4o6VZJl9bLbgKukfQcsAW4uh6ZaWaz1OjrGCJiG9Xw6MmX3Tzp9B7gwrLRzNrNRwiZJeVymiXlcpol5XKaJeVymiXlcpol5XKaJeVymiXlcpol5XKaJeVymiXlcpol5XKaJeVymiXlcpol5XKaJVVkqHS95gpJeyS9KOn+sjHN2qfnJIRJQ6W/QjUW8xlJY/X0gyNrlgE/BC6MiEOSTu1XYLO2KDVU+hpgU0QcAoiI/WVjmrVPk3JONVT69K41y4Hlkp6QNC5pzVQb8txas+ZK7RCaBywDVlENmP61pIXdizy31qy5UkOl9wFjEfFeRLwKvERVVjObpVJDpX9H9ayJpEVUL3NfKZjTrHVKDZV+FDgoaQ+wHfh+RBzsV2izNtCwBrN3Op2YmJgYyn2bDZKknRHRmentfISQWVIup1lSLqdZUi6nWVIup1lSLqdZUi6nWVIup1lSLqdZUi6nWVIup1lSLqdZUi6nWVIup1lSLqdZUsXm1tbrLpcUkmb8f9fM7Gg9yzlpbu0lwAiwTtLIFOsWAN8Dniod0qyNSs2tBbgNuB14t2A+s9YqMrdW0grgzIh4+Fgb8txas+bmvENI0nHAHcBNvdZ6bq1ZcyXm1i4Azgcel/QasBIY804hs7mZ89zaiHg7IhZFxNKIWAqMA5dGhEfrmc1Bqbm1ZlZYz68ABIiIbcC2rstunmbtqrnHMjMfIWSWlMtplpTLaZaUy2mWlMtplpTLaZaUy2mWlMtplpTLaZaUy2mWlMtplpTLaZaUy2mWlMtplpTLaZZUkbm1km6UtEfSbkmPSTqrfFSzdik1t3YX0ImIzwIPAT8pHdSsbYrMrY2I7RHxTn12nGoImJnNQZG5tV3WA4/MJZSZNZwh1JSkq4AOcNE0128ANgAsWbKk5F2bfeiUmFsLgKTVwEaqsZiHp9qQh0qbNTfnubUAki4AfkVVzP3lY5q1T6m5tT8FPg48KOlZSWPTbM7MGioytzYiVhfOZdZ6PkLILCmX0ywpl9MsKZfTLCmX0ywpl9MsKZfTLCmX0ywpl9MsKZfTLCmX0ywpl9MsKZfTLCmX0ywpl9MsqVJzaz8q6YH6+qckLS0d1KxtSs2tXQ8ciohPAj8Hbi8d1Kxtisytrc/fU59+CLhYksrFNGufUnNr/7umnjn0NnBKiYBmbVV0bm0vk+fWAoclvTDI+29gEfDmsENMIWMuZ2ruU7O5UZNyNplbe2TNPknzgJOBg90biohRYBRA0kREdGYTul8yZoKcuZypOUkTs7ldkbm19flv1ae/DvwxImI2gcys0vOZMyLel3Rkbu3xwN1H5tYCExExBvwGuE/SXuAtqgKb2RyUmlv7LvCNGd736AzXD0LGTJAzlzM1N6tc8qtPs5x8+J5ZUn0vZ8ZD/xpkulHSHkm7JT0m6axhZ5q07nJJIWkgeyWb5JJ0Rf14vSjp/mFnkrRE0nZJu+rf4doBZLpb0v7pPh5U5c46825JK3puNCL69kO1A+ll4BzgBOA5YKRrzXeAu+rTVwIPJMj0ZeBj9enrMmSq1y0AdlB9e3inn5lm8FgtA3YBn6jPn5og0yhwXX16BHhtAI/Vl4AVwAvTXL+W6kulBawEnuq1zX4/c2Y89K9npojYHhHv1GfHqT7b7acmjxPAbVTHLb/b5zwzyXUNsCkiDgFE/78CskmmAE6qT58MvN7nTETEDqpPKqZzGXBvVMaBhZJOO9Y2+13OjIf+Nck02Xqqv3j91DNT/TLozIh4uM9ZZpQLWA4sl/SEpHFJaxJkugW4StI+qk8ZbuhzpiZm+u9usIfv/b+RdBXQAS4aco7jgDuAq4eZYxrzqF7arqJ6hbFD0mci4p9DzLQO2BwRP5P0RarP4M+PiH8PMdOM9fuZcyaH/nGsQ/8GnAlJq4GNVN/WfbiPeZpkWgCcDzwu6TWq9yxjA9gp1OSx2geMRcR7EfEq8BJVWYeZaT2wFSAingROpDrudpga/bs7Sp/fJM8DXgHO5n9v3j/dteZ6jt4htDVBpguodjos6/eOhKaZutY/zmB2CDV5rNYA99SnF1G9dDtlyJkeAa6uT59H9Z5TA3i8ljL9DqGvcfQOoad7bm8AgddS/TV9GdhYX3Yr1TMSVH/VHgT2Ak8D5yTI9AfgH8Cz9c/YsDN1rR1IORs+VqJ6yb0HeB64MkGmEeCJurjPAl8dQKYtwBvAe1SvJtYD1wLXTnqcNtWZn2/y+/MRQmZJ+Qghs6RcTrOkXE6zpFxOs6RcTrOkXE6zpFxOs6RcTrOk/gPI6UGUitNMdgAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxejHuRjgOOi"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "HVkqgdt7gOOi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85636113-2efc-4102-bcbb-ca4102f6169e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IZBERI MODEL:\n",
            " 0. Adam_batch-64_epoch-10_learn-0001.torch\n",
            "\n",
            "Vnesi stevilko modela: 0\n",
            "7 ['USA', 'had', 'a', 'world', 'war', 'with', 'germany.']\n",
            "10 [101, 3915, 2018, 1037, 2088, 2162, 2007, 2762, 1012, 102]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at /content/drive/MyDrive/bert/bert-base-uncased/ were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------\n",
            "USA        -> Organization\n",
            "had        -> None\n",
            "a          -> None\n",
            "world      -> None\n",
            "war        -> None\n",
            "with       -> None\n",
            "germany.   -> Geographical Entity\n"
          ]
        }
      ],
      "source": [
        "models = []\n",
        "# Koda za izbiro modela (samo tisti ki so natrenirani vec kot 1 epoch.)\n",
        "print('IZBERI MODEL:')\n",
        "i = 0\n",
        "for path in os.listdir(f'{ROOT}/models'):\n",
        "  if path.endswith('.torch'):\n",
        "    epochs = int(path.split('_')[-2].replace('epoch-', ''))\n",
        "    if epochs > 1:\n",
        "      models.append(f'{ROOT}/models/{path}')\n",
        "      print(f' {i}. {path}')\n",
        "      i+=1\n",
        "\n",
        "model_path = models[int(input(\"\\nVnesi stevilko modela: \"))]\n",
        "\n",
        "# Slovar pomenk tagov v datasetu.\n",
        "tag_meanings = {\n",
        "    'geo': 'Geographical Entity',\n",
        "    'org': 'Organization',\n",
        "    'per': 'Person',\n",
        "    'gpe': 'Geopolitical Entity',\n",
        "    'tim': 'Time indicator',\n",
        "    'art': 'Artifact',\n",
        "    'eve': 'Event',\n",
        "    'nat': 'Natural Phenomenon',\n",
        "    'O': 'None'\n",
        "}\n",
        "\n",
        "meta_data = joblib.load(\"meta.bin\")\n",
        "enc_pos = meta_data[\"enc_pos\"]\n",
        "enc_tag = meta_data[\"enc_tag\"]\n",
        "\n",
        "num_pos = len(list(enc_pos.classes_))\n",
        "num_tag = len(list(enc_tag.classes_))\n",
        "\n",
        "# Definiramo stavek ki ga bomo preverili...\n",
        "\n",
        "sentence = \"\"\"USA had a world war with germany.\"\"\"\n",
        "#sentence = \"\"\"President Bush visited London yesterday while it was snow\"\"\"\n",
        "\n",
        "# Pretvorimo stavek v stevilski zapis\n",
        "tokenized_sentence = config.TOKENIZER.encode(sentence)\n",
        "\n",
        "# Razrezemo stavek na posamezne besede locene z prsledki...\n",
        "sentence = sentence.split()\n",
        "print(len(sentence), sentence)\n",
        "print(len(tokenized_sentence), tokenized_sentence)\n",
        "\n",
        "# Ustvarimo testni dataset z stavkom\n",
        "test_dataset = EntityDataset(\n",
        "    texts=[sentence], \n",
        "    pos=[[0] * len(sentence)], \n",
        "    tags=[[0] * len(sentence)]\n",
        ")\n",
        "\n",
        "# dobimo gpu device\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "# Ustvarimo objekt modela\n",
        "model = EntityModel(num_tag=num_tag, num_pos=num_pos)\n",
        "\n",
        "# Nalozimo shranjeno konfiguracijo v model ki smo jo dobili preko treniranja.\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "# Nalozimo model v gpu.\n",
        "model.to(device)\n",
        "\n",
        "# Aktiviramo model za evaluacijo\n",
        "with torch.no_grad():\n",
        "\n",
        "    # Dobimo evaluacijski podatke.\n",
        "    data = test_dataset[0]\n",
        "    for k, v in data.items():\n",
        "        data[k] = v.to(device).unsqueeze(0)\n",
        "    \n",
        "    # Posljemo testne podatke za evaluacijo v model\n",
        "    tag, pos, _ = model(**data)\n",
        "\n",
        "    # Pretvorimo izhodne podatke v cloveku prijazno obliko...\n",
        "    tags = enc_tag.inverse_transform(\n",
        "            tag.argmax(2).cpu().numpy().reshape(-1)\n",
        "        )[:len(tokenized_sentence)]\n",
        "    position =  enc_pos.inverse_transform(\n",
        "            pos.argmax(2).cpu().numpy().reshape(-1)\n",
        "        )[:len(tokenized_sentence)]\n",
        "\n",
        "    # Izpisemo za vsako besedo kako jo je model labeliral.\n",
        "    print('-------------------------------')\n",
        "    for i in range(len(sentence)):\n",
        "        print(f'{sentence[i]:<10} -> {tag_meanings[tags[i+1].split(\"-\")[-1]]}')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}